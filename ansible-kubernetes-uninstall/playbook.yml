---
- name: Ping Hosts
  hosts: all
  any_errors_fatal: true
  tasks:
    - name: Ping all hosts
      delegate_to: localhost
      delegate_facts: true
      check_mode: false
      ansible.builtin.command: |
        ping -c 2 "{{ inventory_hostname }}"
      become: true
      failed_when: ping_hosts.rc == 1 or ping_hosts.rc > 2
      register: ping_hosts
      changed_when: false

# k3s
- name: K3s | uninstall kubernetes
  hosts: kubernetes
  any_errors_fatal: true
  tasks:
    - name: Check docker secrets encrypted file
      delegate_to: localhost
      ansible.builtin.stat:
        path: "{{ playbook_dir + '/secrets/docker.sops.yaml.enc' }}"
      register: dockersecretsfile
    - name: Unencrypt Docker Secrets
      delegate_to: localhost
      ansible.builtin.set_fact:
        docker: "{{ lookup('file', playbook_dir + '/secrets/docker.sops.yaml.enc') | community.sops.decrypt | ansible.builtin.from_yaml }}"
      when: dockersecretsfile.stat.exists
      failed_when: not dockersecretsfile.stat.exists
    - name: Cleanup | Remove multus-shim if exists
      ansible.builtin.file:
        path: "/opt/cni/bin/multus-shim"
        state: absent
    # - name: Deploy | Install K3s
    #   vars:
    #     k3s_become: true
    #     k3s_state: uninstalled
    #     # k3s_primary_control_node: "{{ groups.controlnodes | first }}"
    #     k3s_registration_address: kubernetes.homelab.heyser.xyz
    #     # Set a specific release version or false for stable, latest for latest
    #     k3s_release_version: false
    #     k3s_build_cluster: true
    #     k3s_etcd_datastore: true
    #     k3s_use_unsupported_config: true
    #     k3s_use_experimental: true
    #     k3s_install_hard_links: true
    #     k3s_registries:
    #       configs:
    #         docker.io:
    #           auth:
    #             username: "{{ (docker | from_yaml).docker_user }}"
    #             password: "{{ (docker | from_yaml).docker_token }}"
    #         index.docker.io:
    #           auth:
    #             username: "{{ (docker | from_yaml).docker_user }}"
    #             password: "{{ (docker | from_yaml).docker_token }}"
    #         registry-1.docker.io:
    #           auth:
    #             username: "{{ (docker | from_yaml).docker_user }}"
    #             password: "{{ (docker | from_yaml).docker_token }}"
    #     k3s_server:
    #       # listen-port: 6443
    #       bind-address: "{{ ansible_host }}"
    #       cluster-cidr: 10.254.0.0/16
    #       service-cidr: 10.255.0.0/16
    #       no-flannel: true
    #       flannel-backend: 'none'
    #       secrets-encryption: true
    #       etcd-expose-metrics: true
    #       disable-network-policy: true
    #       # disable-kube-proxy: true
    #       write-kubeconfig-mode: '0644'
    #       node-taint:
    #         - "node-role.kubernetes.io/control-plane:NoSchedule"
    #       node-label:
    #         - "node-role.kubernetes.io/control-plane=true"
    #         - "topology.kubernetes.io/region=us-east-1"
    #         - "topology.kubernetes.io/zone=us-east-1a"
    #         - "route-reflector=true"
    #       tls-san:
    #         - "kubernetes.homelab.heyser.xyz"
    #       disable:
    #         - traefik
    #         - coredns
    #         - servicelb
    #     k3s_agent:
    #       node-label:
    #         - "topology.kubernetes.io/region=us-east-1"
    #         - "topology.kubernetes.io/zone=us-east-1a"
    #         - "route-reflector=true"
    #   ansible.builtin.import_role:
    #     name: xanmanning.k3s
    #   # when: >-
    #   # not ((ansible_play_hosts_all | difference(ansible_play_hosts)) | length > 0)
    #   register: installk3s
    - name: Setup | Apt Cleanup
      ansible.builtin.apt:
        update_cache: "yes"
        autoclean: "yes"
        autoremove: "yes"
        state: "latest"
      become: true
      changed_when: true
    - name: Ensure that packaging is fixed
      become: true
      ansible.builtin.command: apt --fix-broken install
      environment:
        DEBIAN_FRONTEND: noninteractive
      changed_when: true
    - name: Ensure that packaging is reset
      become: true
      ansible.builtin.command: sudo dpkg --configure -a
      changed_when: true
    - name: Check for rke2-killall.sh
      ansible.builtin.stat:
        path: "/usr/local/bin/rke2-killall.sh"
      register: rke2killall
    - name: Cleanup | Stop Agent Service
      ansible.builtin.command: sudo systemctl stop rke2-agent.service
      when: inventory_hostname in groups["workernodes"]
      changed_when: true
      failed_when: false
    - name: Cleanup | Stop Server Service
      ansible.builtin.command: sudo systemctl stop rke2-server.service
      when: inventory_hostname in groups["controlnodes"]
      changed_when: true
      failed_when: false
    - name: Cleanup | Disable Agent Service
      ansible.builtin.command: sudo systemctl disable rke2-agent.service
      when: inventory_hostname in groups["workernodes"]
      changed_when: true
      failed_when: false
    - name: Cleanup | Disable Server Service
      ansible.builtin.command: sudo systemctl disable rke2-server.service
      when: inventory_hostname in groups["controlnodes"]
      changed_when: true
      failed_when: false
    - name: Cleanup | Run RKE2 Killall
      ansible.builtin.command: rke2-killall.sh
      when: rke2killall.stat.exists
      changed_when: true
      failed_when: false
    - name: Cleanup | Remove reset-flag if exists
      become: true
      ansible.builtin.file:
        path: "/var/lib/rancher/rke2/server/db/reset-flag"
        state: absent
      when: inventory_hostname in groups["controlnodes"]
      changed_when: true
    - name: Check for rke2 config.yaml
      ansible.builtin.stat:
        path: "/etc/rancher/rke2/config.yaml"
      register: rke2config
    - name: Cleanup | Reset Cluster Remove Server From Config
      become: true
      ansible.builtin.command: sudo sed -i '/server:/d' /etc/rancher/rke2/config.yaml
      when: |
        inventory_hostname in groups["controlnodes"] and
        rke2config.stat.exists
      changed_when: true
    - name: Cleanup | Reset Cluster Remove node-external-ip
      become: true
      ansible.builtin.command: sudo sed -i '/node-external-ip:/d' /etc/rancher/rke2/config.yaml
      when: |
        inventory_hostname in groups["controlnodes"] and
        rke2config.stat.exists
      changed_when: true
    - name: Cleanup | Reset Cluster Remove bind
      become: true
      ansible.builtin.command: sudo sed -i '/bind-address:/d' /etc/rancher/rke2/config.yaml
      when: |
        inventory_hostname in groups["controlnodes"] and
        rke2config.stat.exists
      changed_when: true
    - name: Check for rke2 server token
      become: true
      ansible.builtin.stat:
        path: "/var/lib/rancher/rke2/server/token"
      register: rke2servertoken
      when: inventory_hostname in groups["controlnodes"]
    - name: Cleanup | Remove rke2 server token if exists
      become: true
      ansible.builtin.file:
        path: "/var/lib/rancher/rke2/server/token"
        state: absent
      when: |
        inventory_hostname in groups["controlnodes"] and
        rke2servertoken.stat.exists
    - name: Cleanup | Remove Kubelet Config
      become: true
      ansible.builtin.file:
        path: "/etc/kubernetes/kubeletconfig.yml"
        state: absent
    - name: Cleanup | Remove Kubelet Config Alt
      become: true
      ansible.builtin.file:
        path: "/etc/rancher/rke2/kubelet-config.yaml"
        state: absent
    - name: Check for rke2
      ansible.builtin.stat:
        path: "/usr/local/bin/rke2"
      register: rke2binary
    # - name: Cleanup | Reset Cluster
    #   ansible.builtin.command: sudo rke2 server --cluster-reset
    #   when: |
    #     inventory_hostname in groups["controlnodes"] and
    #     rke2binary.stat.exists
    #   register: clusterreset
    #   # failed_when: clusterreset.rc > 0
    #   failed_when: false
    #   changed_when: true
    - name: Cleanup | Unmask Agent Service
      ansible.builtin.command: sudo systemctl unmask rke2-agent.service
      when: inventory_hostname in groups["controlnodes"]
      changed_when: true
      failed_when: false
    - name: Cleanup | Unmask Server Service
      ansible.builtin.command: sudo systemctl unmask rke2-server.service
      when: inventory_hostname in groups["workernodes"]
      changed_when: true
      failed_when: false
    - name: Check for rke2-uninstall.sh
      ansible.builtin.stat:
        path: "/usr/local/bin/rke2-uninstall.sh"
      register: rke2uninstall
    - name: Cleanup | Run RKE2 Uninstall
      ansible.builtin.command: rke2-uninstall.sh
      when: rke2uninstall.stat.exists
      changed_when: true
    - name: Cleanup | Remove multus-shim if exists
      ansible.builtin.file:
        path: "/opt/cni/bin/multus-shim"
        state: absent
    - name: Reload Services | systemctl daemon reload
      ansible.builtin.systemd_service:
        daemon_reload: true
    # - name: Cleanup | Remove multus-shim if exists
    #   ansible.builtin.file:
    #     path: "/opt/cni/bin/multus-shim"
    #     state: absent
    - name: Cleanup | Remove Rancher Config Directory
      become: true
      ansible.builtin.file:
        path: "/etc/rancher"
        state: absent

- name: Reset ControlNodes
  hosts: controlnodes
  gather_facts: false
  tasks:
    - name: Unmount /var/lib/rancher
      ansible.builtin.command: sudo umount /var/lib/rancher
      when: inventory_hostname in groups["controlnodes"]
      changed_when: true
      failed_when: false
    - name: Format /dev/sdb1
      ansible.builtin.command: sudo mkfs.ext4 -F /dev/sdb1
      when: inventory_hostname in groups["controlnodes"]
      changed_when: true
    - name: Mount
      ansible.builtin.command: sudo mount -a
      when: inventory_hostname in groups["controlnodes"]
      changed_when: true
    - name: Set Permissions
      ansible.builtin.command: sudo chmod -R a+w /var/lib/rancher
      when: inventory_hostname in groups["controlnodes"]
      changed_when: true

# upgrades
- name: Setup | Apt Update/Upgrade
  hosts: kubernetes
  any_errors_fatal: true
  gather_facts: false
  tasks:
    - name: Setup | Ensure dpkg is configured
      become: true
      ansible.builtin.command: dpkg --configure -a
      changed_when: true
    - name: Setup | Apt Update/Upgrade
      ansible.builtin.apt:
        update_cache: "yes"
        upgrade: "yes"
        autoclean: "yes"
        autoremove: "yes"
        state: "latest"
      become: true
      changed_when: true

# database Cleanup
- name: Database Cleanup
  hosts: localhost
  any_errors_fatal: true
  gather_facts: false
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Ensure python3-psycopg2 is installed
      ansible.builtin.package:
        name: python3-psycopg2
        state: present
    - name: Stat age.key
      ansible.builtin.stat:
        path: "{{ home }}/.config/sops/age/keys.txt"
      register: age_key_file
    - name: Fail if Age Keyfile does not exist
      ansible.builtin.fail:
        message: "{{ home }}/.config/sops/age/keys.txt does not exist"
      when: not age_key_file.stat.exists
    - name: Get Age Public Key
      ansible.builtin.command: |
        grep -Po '(?<=public key: ).*' {{ home }}/.config/sops/age/keys.txt
      register: age_public_key
      when: age_key_file.stat.exists
      changed_when: true
    - name: Debug Age Public Key
      ansible.builtin.debug:
        msg: "age public key: \"{{ age_public_key.stdout }}\""
    - name: Get External Datastore Postgres Secrets
      changed_when: true
      ansible.builtin.set_fact:
        rke2_etcd_external_postgres_address: "{{ (lookup('file', playbook_dir + '/secrets/rke2.sops.yaml.enc') | community.sops.decrypt | ansible.builtin.from_yaml).RKE2_ETCD_EXTERNAL_POSTGRES_ADDRESS }}"
        rke2_etcd_external_postgres_port: "{{ (lookup('file', playbook_dir + '/secrets/rke2.sops.yaml.enc') | community.sops.decrypt | ansible.builtin.from_yaml).RKE2_ETCD_EXTERNAL_POSTGRES_PORT }}"
        rke2_etcd_external_postgres_database: "{{ (lookup('file', playbook_dir + '/secrets/rke2.sops.yaml.enc') | community.sops.decrypt | ansible.builtin.from_yaml).RKE2_ETCD_EXTERNAL_POSTGRES_DATABASE }}"
        rke2_etcd_external_postgres_username: "{{ (lookup('file', playbook_dir + '/secrets/rke2.sops.yaml.enc') | community.sops.decrypt | ansible.builtin.from_yaml).RKE2_ETCD_EXTERNAL_POSTGRES_USERNAME }}"
        rke2_etcd_external_postgres_password: "{{ (lookup('file', playbook_dir + '/secrets/rke2.sops.yaml.enc') | community.sops.decrypt | ansible.builtin.from_yaml).RKE2_ETCD_EXTERNAL_POSTGRES_PASSWORD }}"
    - name: Drop rke2 database
      community.postgresql.postgresql_db:
        state: absent
        name: "{{ rke2_etcd_external_postgres_database }}"
        login_user: "{{ rke2_etcd_external_postgres_username }}"
        login_password: "{{ rke2_etcd_external_postgres_password }}"
        login_host: "{{ rke2_etcd_external_postgres_address }}"
        login_port: "{{ rke2_etcd_external_postgres_port }}"
      changed_when: true

# reboot
- name: Reboot
  hosts: kubernetes
  any_errors_fatal: true
  gather_facts: false
  # serial: 1
  tasks:
    - name: Reboot all nodes
      ansible.builtin.reboot:
