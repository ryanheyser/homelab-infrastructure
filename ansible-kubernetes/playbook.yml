---
# Load Variables
- name: Load Variables
  hosts: all
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Stat vars.yml
      ansible.builtin.stat:
        path: "{{ playbook_dir }}/vars.yml"
      register: vars_file
    - name: Load vars
      ansible.builtin.include_vars:
        file: "{{ vars_file.stat.path }}"
      when: vars_file.stat.exists

- name: Ping Hosts
  hosts: all
  tasks:
    - name: Ping all hosts
      delegate_to: localhost
      delegate_facts: true
      check_mode: false
      ansible.builtin.command: |
        ping -c 2 "{{ inventory_hostname }}"
      become: true
      failed_when: ping_hosts.rc == 1 or ping_hosts.rc > 2
      register: ping_hosts
      changed_when: false

- name: Setup SSH Access
  hosts: all
  tasks:
    - name: Setup | Read SSH Key
      ansible.builtin.set_fact:
        ansible_ssh_key_pub: "{{ lookup('template', playbook_dir + '/secrets/ansible_id_ed25519.pub') }}"
    - name: Setup | Add SSH Key to known hosts
      ansible.builtin.known_hosts:
        name: "{{ ansible_host }}"
        key: "{{ ansible_host }} {{ ansible_ssh_key_pub }}"
      changed_when: false

- name: Setup All Hosts
  hosts: all
  connection: ssh
  gather_facts: true
  tasks:
    # - name: Setup | apt update
    #   ansible.builtin.command: apt update -y
    #   become: true
    #   changed_when: true
    # - name: Setup | apt upgrade
    #   ansible.builtin.command: apt upgrade -y
    #   become: true
    #   changed_when: true
    - name: Setup | Apt Update/Upgrade
      ansible.builtin.apt:
        update_cache: "yes"
        upgrade: "yes"
        autoclean: "yes"
        autoremove: "yes"
        state: "latest"
      changed_when: true
    - name: Setup | install required packages
      ansible.builtin.apt:
        state: 'present'
        update_cache: true
        pkg:
          - age 
          - build-essential 
          - curl 
          - dnsutils
          - linux-generic
          - linux-headers-generic 
          - mtr-tiny 
          - net-tools
          - nfs-common
          - open-vm-tools
          - openssl 
          - p7zip-full 
          - pciutils 
          - python3 
          - python3-pip 
          - python3-venv 
          - python3-virtualenv
          - software-properties-common 
          - tcpdump 
          - vim 
          - zstd
      become: true
      changed_when: true
    - name: Setup | install ansible ppa
      ansible.builtin.apt_repository:
        repo: ppa:ansible/ansible
    - name: Setup | install ansible packages
      ansible.builtin.apt:
        state: 'present'
        update_cache: true
        pkg:
          - ansible
      become: true
      changed_when: true
    - name: Install sops on localhost
      ansible.builtin.include_role:
        name: community.sops.install
        vars_from: "{{ vars_file.stat.path }}"
      vars:
        sops_install_on_localhost: true
        sops_github_token: "{{ github_token }}"
        sops_become_on_install: true
      when: >-
        not ((ansible_play_hosts_all | difference(ansible_play_hosts)) | length > 0) and
        vars_file.stat.exists
    - name: Install sops 
      ansible.builtin.include_role:
        name: community.sops.install
        vars_from: "{{ vars_file.stat.path }}"
      vars:
        sops_github_token: "{{ github_token }}"
        sops_become_on_install: true
      when: >-
        not ((ansible_play_hosts_all | difference(ansible_play_hosts)) | length > 0) and
        vars_file.stat.exists
    - name: Set hostname on remote hosts
      ansible.builtin.hostname:
        name: "{{ ansible_fqdn }}"
      when: >-
        not ((ansible_play_hosts_all | difference(ansible_play_hosts)) | length > 0)

- name: Setup Kubernetes Hosts
  hosts: kubernetes
  connection: ssh
  # vars:
  tasks:
    - name: Setup | upgrade all packages
      ansible.builtin.apt:
        state: latest # noqa package-latest
        update_cache: true
        name: '*'
      become: true
    - name: Setup | install required packages
      ansible.builtin.apt:
        state: 'present'
        update_cache: true
        pkg:
          - bind9 
          - bind9-utils 
          - bind9-dnsutils 
          - containernetworking-plugins
      become: true
      changed_when: true
      register: packageinstall

# Age/Sops
- name: Age/Sops
  hosts: localhost
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Stat age.key
      ansible.builtin.stat:
        path: "{{ home }}/.config/sops/age/keys.txt"
      register: age_key_file
    - name: Fail if Age Keyfile does not exist
      ansible.builtin.fail:
        message: "{{ home }}/.config/sops/age/keys.txt does not exist"
      when: not age_key_file.stat.exists
    - name: Get Age Public Key
      ansible.builtin.command: |
        grep -Po '(?<=public key: ).*' {{ home }}/.config/sops/age/keys.txt
      register: age_public_key
      when: age_key_file.stat.exists
      changed_when: true

# Flux
- name: Install Flux
  hosts: localhost
  tasks:
    - name: Install Flux CLI
      ansible.builtin.shell:
        cmd: |
          set -o pipefail
          curl -s https://fluxcd.io/install.sh | bash
        executable: /bin/bash
      become: true
      changed_when: true

# DNS
- name: Externaldns-key
  hosts: localhost
  strategy: linear
  tasks:
    - name: Generate externaldns-key
      # noqa: run-once[task]
      run_once: true
      ansible.builtin.shell:
        cmd: |
          set -o pipefail
          /usr/sbin/tsig-keygen -a hmac-sha256 externaldns | /usr/bin/grep secret | /usr/bin/cut -d '"' -f2
        executable: /bin/bash
      register: externaldns_key
      changed_when: false
- name: Setup DNS
  hosts: kubernetes
  tasks:
    - name: Setup BIND9
      vars:
        bind_dnssec_enable: false
        bind_dnssesc_validation: false
        bind_listen_ipv4:
          - any
        bind_allow_query:
          - any
        bind_allow_recursion:
          - any
        bind_dns_keys:
          - name: externaldns-key
            algorithm: hmac-sha256
            secret: "{{ externaldns_key }}"
        bind_zones:
          - name: homelab.heyser.lan
            type: primary
            primaries:
              "{{ groups.controlnodes | map('extract', hostvars) | map(attribute='ansible_host') | list }}"
          - name: homelab.heyser.lan
            type: secondary
            primaries:
              "{{ groups.workernodes | map('extract', hostvars) | map(attribute='ansible_host') | list }}"
      ansible.builtin.import_role:
        name: "bertvv.bind"
      when: >-
        packageinstall.stdout is defined and
        packageinstall.rc == 0 and
        not packageinstall is skipped

# k3s
- name: K3s | install kubernetes
  hosts: kubernetes
  tasks:
    - name: Check docker secrets encrypted file
      delegate_to: localhost
      ansible.builtin.stat:
        path: "{{ playbook_dir + '/secrets/docker.sops.yaml.enc' }}"
      register: dockersecretsfile
    - name: Unencrypt Docker Secrets
      delegate_to: localhost
      ansible.builtin.set_fact:
          docker: "{{ lookup('file', playbook_dir + '/secrets/docker.sops.yaml.enc') | community.sops.decrypt | ansible.builtin.from_yaml }}"
      when: dockersecretsfile.stat.exists
      failed_when: not dockersecretsfile.stat.exists
    - name: Deploy | Install K3s
      vars:
        k3s_become: true
        k3s_state: installed
        # k3s_primary_control_node: "{{ groups.controlnodes | first }}"
        k3s_registration_address: kubernetes.homelab.heyser.xyz
        # Set a specific release version or false for stable, latest for latest
        k3s_release_version: false
        k3s_build_cluster: true
        k3s_etcd_datastore: true
        k3s_use_unsupported_config: true
        k3s_use_experimental: true
        k3s_install_hard_links: true
        k3s_registries:
          mirrors:
            docker.io:
            index.docker.io:
            registry-1.docker.io:
          configs:
            docker.io:
              auth:
                username: "{{ (docker | from_yaml).docker_user }}"
                password: "{{ (docker | from_yaml).docker_token }}"
            index.docker.io:
              auth:
                username: "{{ (docker | from_yaml).docker_user }}"
                password: "{{ (docker | from_yaml).docker_token }}"
            registry-1.docker.io:
              auth:
                username: "{{ (docker | from_yaml).docker_user }}"
                password: "{{ (docker | from_yaml).docker_token }}"
        k3s_server:
          # listen-port: 6443
          bind-address: "{{ ansible_host }}"
          # bind-address: 0.0.0.0
          cluster-cidr: 10.254.0.0/16
          service-cidr: 10.255.0.0/16
          # advertise-address: 10.255.0.1
          # advertise-port: 443
          no-flannel: true
          flannel-backend: 'none'
          secrets-encryption: true
          etcd-expose-metrics: true
          disable-network-policy: true
          disable-kube-proxy: false
          embedded-registry: true
          write-kubeconfig-mode: '0644'
          etcd-arg: "--election-timeout 10000"
          node-taint:
            - "node-role.kubernetes.io/control-plane:NoSchedule"
          node-label:
            - "node-role.kubernetes.io/control-plane=true"
            - "topology.kubernetes.io/region=us-east-1"
            - "topology.kubernetes.io/zone=us-east-1a"
            - "route-reflector=true"
          tls-san:
            - "kubernetes.homelab.heyser.xyz"
          disable:
            - traefik
            - coredns
            - servicelb
        k3s_agent:
          node-label:
            - "topology.kubernetes.io/region=us-east-1"
            - "topology.kubernetes.io/zone=us-east-1a"
            - "route-reflector=true"
      ansible.builtin.import_role:
        name: xanmanning.k3s
          #when: >-
          #not ((ansible_play_hosts_all | difference(ansible_play_hosts)) | length > 0)
      register: installk3s
    # - name: Cleanup | Remove multus-shim if exists
    #   ansible.builtin.file:
    #     path: "/opt/cni/bin/multus-shim"
    #     state: absent
- name: Post k3s install
  hosts: controlnodes
  gather_facts: false
  strategy: linear
  tasks:
    - name: Download kubeconfig
      # noqa: run-once[task]
      run_once: true
      become: true
      ansible.builtin.fetch:
        src: /etc/rancher/k3s/k3s.yaml
        dest: "{{ playbook_dir }}/k3s.yaml"
        flat: true
          # when: >-
          #installk3s is defined
- name: Post k3s install p2
  hosts: localhost
  gather_facts: false
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Check if k3s.yaml File Exists
      ansible.builtin.stat:
        path: "{{ playbook_dir }}/k3s.yaml"
      register: k3s_yaml
        # when: >-
        # installk3s is defined
    - name: Create system kube directory
      ansible.builtin.file:
        path: "/etc/rancher/k3s"
        state: directory
        mode: '0755'
          #when: >-
          #installk3s is defined
    - name: Create user kube directory
      vars:
        home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
      ansible.builtin.file:
        path: "{{ home }}/.kube"
        state: directory
        mode: '0755'
          #when: >-
          #installk3s is defined
    - name: Copy Kubeconfig to system
      ansible.builtin.copy:
        src: "{{ playbook_dir }}/k3s.yaml"
        dest: "/etc/rancher/k3s/"
        force: true
        mode: '0644'
      when: >-
        k3s_yaml.stat.exists
    - name: Copy Kubeconfig to user default
      vars:
        home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
      ansible.builtin.command: |
        /usr/bin/cp "{{ playbook_dir }}/k3s.yaml" "{{ home }}/.kube/config"
      changed_when: true
      when: >-
        k3s_yaml.stat.exists
      #installk3s is defined and

- name: Post k3s install p3
  hosts: kubernetes
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Check for kubeconfig
      delegate_to: localhost
      ansible.builtin.stat:
        path: "{{ home }}/.kube/config"
      register: kubeconfig
    - name: Wait for all nodes
      delegate_to: localhost
      kubernetes.core.k8s_info:
        kubeconfig: "{{ home }}/.kube/config"
        kind: Node
        name: "{{ ansible_host }}"
        wait: true
        wait_condition:
          type: "Ready"
          status: "True"
        wait_sleep: 5
        wait_timeout: 300
    # - name: Patch nodes to add route-reflector
    #   delegate_to: localhost
    #   kubernetes.core.k8s:
    #     kubeconfig: "{{ home }}/.kube/config"
    #     state: patched
    #     kind: Node
    #     name: "{{ ansible_host }}"
    #     definition:
    #       metadata:
    #         annotations:
    #           projectcalico.org/RouteReflectorClusterID: 244.0.0.1

# Reflector
- name: Reflector
  hosts: localhost
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Check for kubeconfig
      ansible.builtin.stat:
        path: "{{ home }}/.kube/config"
      register: kubeconfig
    - name: Download reflector manifest to the cluster.
      ansible.builtin.get_url:
        url: https://github.com/emberstack/kubernetes-reflector/releases/latest/download/reflector.yaml
        dest: "{{ playbook_dir }}/reflector.yaml"
        mode: '0664'
      register: reflector
    - name: Apply reflector manifest to the cluster.
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        state: present
        src: "{{ playbook_dir }}/reflector.yaml"
      register: reflectorapply      
    
# Docker Registry Credentials
- name: Create docker hub registry credential
  hosts: localhost
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Check docker secrets encrypted file
      ansible.builtin.stat:
        path: "{{ playbook_dir + '/secrets/docker.sops.yaml.enc' }}"
      register: dockersecretsfile
    - name: Unencrypt Docker Secrets
      ansible.builtin.set_fact:
          docker: "{{ lookup('file', playbook_dir + '/secrets/docker.sops.yaml.enc') | community.sops.decrypt | ansible.builtin.from_yaml }}"
      when: dockersecretsfile.stat.exists
      failed_when: not dockersecretsfile.stat.exists
    - name: Format docker config
      ansible.builtin.command: |
        echo -n '{"auths":{"https://index.docker.io/v1/":{"username":"{{ (docker | from_yaml).docker_user }}","email":"{{ (docker | from_yaml).docker_email }}","auth":"{{ (docker | from_yaml).docker_token }}"}}}'
      register: dockercfg
      changed_when: true
      failed_when:
        - dockercfg.rc != 0
        - dockercfg.stdout == ""
    - name: Create Secret
      when: kubeconfig.stat.exists
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        state: present
        definition:
          apiVersion: v1
          kind: Secret
          metadata:
            name: docker-registry-credentials
            namespace: kube-system
            annotations:
              reflector.v1.k8s.emberstack.com/reflection-allowed: "true"
              reflector.v1.k8s.emberstack.com/reflection-auto-enabled: "true"
          type: kubernetes.io/dockercfg
          data:
            .dockercfg: "{{ dockercfg.stdout | b64encode }}"

# CNI
- name: Configure CNI
  hosts: localhost
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Get Latest Calico Release
      ansible.builtin.uri:
        url: https://api.github.com/repos/projectcalico/calico/releases/latest
        return_content: true
      register: calico_release_json
      failed_when: calico_release_json is failed
    - name: Get Latest Multus Release
      ansible.builtin.uri:
        url: https://api.github.com/repos/k8snetworkplumbingwg/multus-cni/releases/latest
        return_content: true
      register: multus_release_json
      failed_when: multus_release_json is failed
    - name: Download tigera operator manifest to the cluster.
      ansible.builtin.get_url:
        url: https://raw.githubusercontent.com/projectcalico/calico/{{ calico_release_json.json.tag_name }}/manifests/tigera-operator.yaml
        dest: "{{ playbook_dir }}/tigera-operator.yaml"
        mode: '0664'
      register: tigeraoperatormanifest
    - name: Download tigera apiserver manifest to the cluster.
      ansible.builtin.get_url:
        url: https://raw.githubusercontent.com/projectcalico/calico/{{ calico_release_json.json.tag_name }}/manifests/apiserver.yaml
        dest: "{{ playbook_dir }}/apiserver.yaml"
        mode: '0664'
      register: tigeraapiservermanifest
    - name: Download multus daemonset thick manifest to the cluster.
      ansible.builtin.get_url:
        url: https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/{{ multus_release_json.json.tag_name }}/deployments/multus-daemonset-thick.yml
        dest: "{{ playbook_dir }}/multus-daemonset-thick.yml"
        mode: '0664'
      register: multusdaemonsetmanifest
    - name: Download calicoctl to the cluster.
      ansible.builtin.get_url:
        url: https://github.com/projectcalico/calico/releases/download/{{ calico_release_json.json.tag_name }}/calicoctl-linux-amd64
        dest: "/usr/local/bin/kubectl-calico"
        mode: '0777'
      register: calicoctl
    - name: Check for kubeconfig
      ansible.builtin.stat:
        path: "{{ home }}/.kube/config"
      register: kubeconfig
    - name: Apply tigera operator manifest to the cluster.
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        state: present
        src: "{{ playbook_dir }}/tigera-operator.yaml"
    - name: Apply multus daemonset manifest to the cluster.
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        state: present
        src: "{{ playbook_dir }}/multus-daemonset-thick.yml"
      register: multusdaemonsetmanifestapply      
    - name: Wait for apply tigera operator manifest to complete
      kubernetes.core.k8s_info:
        kubeconfig: "{{ home }}/.kube/config"
        kind: Deployment
        namespace: tigera-operator
        label_selectors:
          - k8s-app = tigera-operator
        wait: true
        wait_condition:
          type: "Available"
          status: "True"
        wait_sleep: 5
        wait_timeout: 300
    - name: Create Installation Configuration for Tigera Operator
      when: kubeconfig.stat.exists
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        state: present
        # apply: true
        # server_side_apply: 
        #   field_manager: ansible
        definition:
          kind: Installation
          apiVersion: operator.tigera.io/v1
          metadata:
            name: default
          spec:
            serviceCIDRs:
              - 10.255.0.0/16
            calicoNetwork:
              # linuxDataplane: "BPF"
              bgp: "Enabled"
              # mtu: 9000
              nodeAddressAutodetectionV4:
                cidrs:
                - 10.0.0.0/18
              ipPools:
              - name: default
                cidr: 10.254.0.0/16
                encapsulation: None
                natOutgoing: Disabled
                nodeSelector: all()
                allowedUses:
                - Workload
                - Tunnel
    - name: Create APIServer for Tigera Operator
      when: kubeconfig.stat.exists
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        state: present
        apply: true
        server_side_apply: 
          field_manager: ansible
        definition:
          apiVersion: operator.tigera.io/v1
          kind: APIServer
          metadata:
            name: default
          spec: {}
    # - name: Wait until HTTP status is 401
    #   ansible.builtin.uri:
    #     url: "https://{{ hostvars[groups['controlnodes'][0]]['ansible_host'] }}:6443"
    #     return_content: yes
    #     validate_certs: no
    #     status_code:
    #       - 401
    #   until: uri_output.status == 401
    #   retries: 20 # Retries for 20 * 15 seconds = 300 seconds = 5 minutes
    #   delay: 15 # Every 15 seconds
    #   register: uri_output
    - name: Wait for create calico-kube-controllers to complete
      kubernetes.core.k8s_info:
        kubeconfig: "{{ home }}/.kube/config"
        kind: Deployment
        namespace: calico-system
        label_selectors:
          - k8s-app = calico-kube-controllers
        wait: true
        wait_condition:
          type: "Available"
          status: "True"
        wait_sleep: 5
        wait_timeout: 300
      register: calicokubecontrollersstatus
      until: calicokubecontrollersstatus.resources[0].status.readyReplicas | int > 0
      retries: 20
      delay: 15
    - name: Create calico-apiserver namespace
      when: kubeconfig.stat.exists
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        name: calico-apiserver
        api_version: v1
        kind: Namespace
        state: present
        definition:
          metadata:
            # annotations:
            #   cni.projectcalico.org/ipv4pools: '["system"]'
            labels:
              name: calico-apiserver
    - name: Apply tigera apiserver manifest to the cluster.
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        state: present
        src: "{{ playbook_dir }}/apiserver.yaml"
    - name: Wait for apply tigera apiserver manifest to complete
      kubernetes.core.k8s_info:
        kubeconfig: "{{ home }}/.kube/config"
        kind: Deployment
        namespace: calico-apiserver
        label_selectors:
          - k8s-app = calico-apiserver
        wait: true
        wait_condition:
          type: "Available"
          status: "True"
        wait_sleep: 30
        wait_timeout: 600
      register: calicoapiserverstatus
      until: calicoapiserverstatus.resources[0].status.readyReplicas | int > 0
      retries: 20
      delay: 15
    - name: Wait for apiservice apiserver to be available
      kubernetes.core.k8s_info:
        kubeconfig: "{{ home }}/.kube/config"
        kind: APIService
        name: v3.projectcalico.org
        wait: true
        wait_condition:
          type: "Available"
          status: "True"
        wait_sleep: 30
        wait_timeout: 600
      register: calicoapiservicestatus
      until: calicoapiservicestatus.resources[0].status.conditions[0].status == "True"
      retries: 20
      delay: 15
    # - name: Create Default IPPool Config
    #   when: kubeconfig.stat.exists
    #   kubernetes.core.k8s:
    #     kubeconfig: "{{ home }}/.kube/config"
    #     state: present
    #     apply: true
    #     server_side_apply:
    #       field_manager: ansible
    #     definition:
    #       apiVersion: projectcalico.org/v3
    #       kind: IPPool
    #       metadata:
    #         name: default
    #         namespace: kube-system
    #       spec:
    #         cidr: 10.0.101.0/24
    #         ipipMode: Never
    #         disabled: false
    #         disableBGPExport: false
    #         natOutgoing: false
    #         nodeSelector: all()
    #         allowedUses:
    #         - Workload
    #         - Tunnel
    # - name: Create System IPPool Config
    #   when: kubeconfig.stat.exists
    #   kubernetes.core.k8s:
    #     kubeconfig: "{{ home }}/.kube/config"
    #     state: present
    #     apply: true
    #     server_side_apply:
    #       field_manager: ansible
    #     definition:
    #       apiVersion: projectcalico.org/v3
    #       kind: IPPool
    #       metadata:
    #         name: system
    #         namespace: kube-system
    #       spec:
    #         cidr: 10.0.100.0/24
    #         ipipMode: Never
    #         disabled: false
    #         disableBGPExport: false
    #         natOutgoing: false
    #         nodeSelector: all()
    #         allowedUses:
    #         - Workload
    #         - Tunnel
    - name: Create Global BGP Config
      when: kubeconfig.stat.exists
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        state: present
        # apply: true
        # server_side_apply:
        #   field_manager: ansible
        definition:
          apiVersion: projectcalico.org/v3
          kind: BGPConfiguration
          metadata:
            name: default
            namespace: kube-system
          spec:
            asNumber: 65001
            serviceLoadBalancerIPs:
              - cidr: 10.0.101.0/24
              - cidr: 10.0.100.0/24
            # communities:
            #   - name: bgpdefault        
    - name: Create Global BGP Peer Config
      when: kubeconfig.stat.exists
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        state: present
        # apply: true
        # server_side_apply:
        #   field_manager: ansible
        definition:
          apiVersion: projectcalico.org/v3
          kind: BGPPeer
          metadata:
            name: bgppeer
            namespace: kube-system
          spec:
            peerIP: 10.0.0.1
            asNumber: 65000

# Metallb
- name: Metallb Pre-steps
  hosts: localhost
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Check for kubeconfig
      ansible.builtin.stat:
        path: "{{ home }}/.kube/config"
      register: kubeconfig
    - name: Add Metallb repository
      when: kubeconfig.stat.exists
      kubernetes.core.helm_repository:
        kubeconfig: "{{ home }}/.kube/config"
        name: metallb
        repo_url: https://metallb.github.io/metallb
    - name: Create network-system namespace
      when: kubeconfig.stat.exists
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        name: network-system
        api_version: v1
        kind: Namespace
        state: present
        definition:
          metadata:
            annotations:
              # cni.projectcalico.org/ipv4pools: '["system"]'
              metallb.universe.tf/address-pool: "system"
            labels:
              pod-security.kubernetes.io/enforce: privileged
              pod-security.kubernetes.io/audit: privileged
              pod-security.kubernetes.io/warn: privileged
- name: Metallb Install
  hosts: localhost
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Check for kubeconfig
      ansible.builtin.stat:
        path: "{{ home }}/.kube/config"
      register: kubeconfig
    - name: Deploy Metallb chart
      when: kubeconfig.stat.exists
      kubernetes.core.helm:
        # renovate: registryUrl=https://metallb.github.io/metallb
        kubeconfig: "{{ home }}/.kube/config"
        name: metallb
        release_namespace: network-system
        chart_ref: metallb/metallb
        chart_version: 0.14.8
        # values:
    - name: Wait for metallb controller to complete
      kubernetes.core.k8s_info:
        kubeconfig: "{{ home }}/.kube/config"
        kind: Deployment
        namespace: network-system
        label_selectors:
          - app.kubernetes.io/name = metallb
          - app.kubernetes.io/component = controller
        wait: true
        wait_condition:
          type: "Available"
          status: "True"
        wait_sleep: 30
        wait_timeout: 600
      register: metallbcontrollerstatus
      until: metallbcontrollerstatus.resources[0].status.readyReplicas | int > 0
      retries: 20
      delay: 15
    - name: Create Default IPAddressPool Config
      when: kubeconfig.stat.exists
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        state: present
        # apply: true
        # server_side_apply:
        #   field_manager: ansible
        definition:
          apiVersion: metallb.io/v1beta1
          kind: IPAddressPool
          metadata:
            name: default
            namespace: network-system
          spec:
            addresses:
            - 10.0.101.0/24
    - name: Create System IPAddressPool Config
      when: kubeconfig.stat.exists
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        state: present
        # apply: true
        # server_side_apply:
        #   field_manager: ansible
        definition:
          apiVersion: metallb.io/v1beta1
          kind: IPAddressPool
          metadata:
            name: system
            namespace: network-system
          spec:
            addresses:
            - 10.0.100.0/24

# Coredns
- name: Coredns Pre-steps
  hosts: localhost
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Check for kubeconfig
      ansible.builtin.stat:
        path: "{{ home }}/.kube/config"
      register: kubeconfig
    - name: Add coredns repository
      when: kubeconfig.stat.exists
      kubernetes.core.helm_repository:
        kubeconfig: "{{ home }}/.kube/config"
        name: coredns
        repo_url: https://coredns.github.io/helm
    - name: Create network-system namespace
      when: kubeconfig.stat.exists
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        name: network-system
        api_version: v1
        kind: Namespace
        state: patched
        definition:
          annotations:
            # cni.projectcalico.org/ipv4pools: '["system"]'
            metallb.universe.tf/address-pool: "system"
- name: Coredns Install
  hosts: localhost
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Check for kubeconfig
      ansible.builtin.stat:
        path: "{{ home }}/.kube/config"
      register: kubeconfig
    - name: Deploy Coredns chart
      when: kubeconfig.stat.exists
      kubernetes.core.helm:
        kubeconfig: "{{ home }}/.kube/config"
        name: coredns
        release_namespace: kube-system
        # renovate: registryUrl=https://coredns.github.io/helm
        chart_ref: coredns/coredns
        chart_version: 1.32.0
        values:
          servers:
          - zones:
            - zone: .
            port: 53
            plugins:
            - name: errors
            # Serves a /health endpoint on :8080, required for livenessProbe
            - name: health
              configBlock: |-
                lameduck 5s
            # Serves a /ready endpoint on :8181, required for readinessProbe
            - name: ready
            # Required to query kubernetes API for data
            - name: kubernetes
              parameters: cluster.local in-addr.arpa ip6.arpa homelab.heyser.lan homelab.heyser.xyz
              configBlock: |-
                pods insecure
                fallthrough in-addr.arpa ip6.arpa homelab.heyser.lan homelab.heyser.xyz
                ttl 30
            - name: forward
              parameters: . 10.0.100.3 10.0.0.1
              configBlock: |-
                force_tcp
                except "cluster.local" "svc.cluster.local" ".svc"
                policy sequential
            - name: cache
              parameters: 30
            - name: loop
            - name: reload
            - name: loadbalance

# Traefik
- name: Traefik Pre-steps
  hosts: localhost
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Check for kubeconfig
      ansible.builtin.stat:
        path: "{{ home }}/.kube/config"
      register: kubeconfig
    - name: Add traefik repository
      when: kubeconfig.stat.exists
      kubernetes.core.helm_repository:
        kubeconfig: "{{ home }}/.kube/config"
        name: traefik
        repo_url: https://helm.traefik.io/traefik
    - name: Create network-system namespace
      when: kubeconfig.stat.exists
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        name: network-system
        api_version: v1
        kind: Namespace
        state: present
        definition:
          annotations:
            # cni.projectcalico.org/ipv4pools: '["system"]'
    - name: Create Traefik IP Reservation
      when: kubeconfig.stat.exists
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        state: present
        apply: true
        server_side_apply:
          field_manager: ansible
          force_conflicts: true
        definition:
          apiVersion: projectcalico.org/v3
          kind: IPReservation
          metadata:
            name: traefik-ipv4-reservation
            namespace: network-system
          spec:
            reservedCIDRs:
              - 10.0.100.1
- name: Install Traefik CRDs
  hosts: localhost
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Check for kubeconfig
      ansible.builtin.stat:
        path: "{{ home }}/.kube/config"
      register: kubeconfig
    - name: Get Latest Traefik Release
      ansible.builtin.uri:
        url: https://api.github.com/repos/traefik/traefik/releases/latest
        return_content: true
      register: traefik_release_json
      failed_when: traefik_release_json is failed
    - name: Download Traefik CRD Definition
      ansible.builtin.get_url:
        url: https://raw.githubusercontent.com/traefik/traefik/{{ traefik_release_json.json.tag_name }}/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml
        dest: "{{ playbook_dir }}/kubernetes-crd-definition-v1.yml"
        mode: '0664'
      register: traefikcrddefv1
    - name: Download Traefik CRD RBAC Definition
      ansible.builtin.get_url:
        url: https://raw.githubusercontent.com/traefik/traefik/{{ traefik_release_json.json.tag_name }}/docs/content/reference/dynamic-configuration/kubernetes-crd-rbac.yml
        dest: "{{ playbook_dir }}/kubernetes-crd-rbac.yml"
        mode: '0664'
      register: traefikcrdrbacdef
    # - name: Download Traefik CRD Resource Definition
    #   ansible.builtin.get_url:
    #     url: https://raw.githubusercontent.com/traefik/traefik/{{ traefik_release_json.json.tag_name }}/docs/content/reference/dynamic-configuration/kubernetes-crd-resource.yml
    #     dest: "{{ playbook_dir }}/kubernetes-crd-resource.yml"
    #     mode: '0664'
    #   register: traefikcrdresourcedef
    - name: Install Traefik CRD Definition
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        src: "{{ traefikcrddefv1.dest }}"
        namespace: network-system
        state: present
        apply: true
        force: true
        server_side_apply:
          field_manager: ansible
          force_conflicts: true
      when: >-
        traefikcrddefv1 is not none and
        kubeconfig.stat.exists
    - name: Install Traefik CRD RBAC
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        src: "{{ traefikcrdrbacdef.dest }}"
        namespace: network-system
        state: present
        apply: true
        force: true
        server_side_apply:
          field_manager: ansible
          force_conflicts: true
      when: >-
        traefikcrdrbacdef is not none and
        kubeconfig.stat.exists
    # - name: Install Traefik CRD Resource
    #   kubernetes.core.k8s:
    #     kubeconfig: "{{ home }}/.kube/config"
    #     src: "{{ traefikcrdresourcedef.dest }}"
    #     namespace: network-system
    #     state: present
    #     apply: true
    #     force: true
    #     server_side_apply:
    #       field_manager: ansible
    #       force_conflicts: true
    #   when: >-
    #     traefikcrdresourcedef is not none and
    #     kubeconfig.stat.exists
- name: Install Traefik
  hosts: localhost
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Check for kubeconfig
      ansible.builtin.stat:
        path: "{{ home }}/.kube/config"
      register: kubeconfig
    - name: Check for traefik values
      ansible.builtin.stat:
        path: "{{ playbook_dir }}/helm_values/traefik/values.yaml"
      register: traefikvalues
    - name: Deploy Traefik chart
      when: kubeconfig.stat.exists
      kubernetes.core.helm:
        kubeconfig: "{{ home }}/.kube/config"
        name: traefik
        release_namespace: network-system
        # renovate: registryUrl=https://helm.traefik.io/traefik
        chart_ref: traefik/traefik
        chart_version: 31.0.0
        values_files:
          - "{{ traefikvalues.stat.path }}"
    - name: Check for crowdsec bouncer config
      ansible.builtin.stat:
        path: "{{ playbook_dir }}/helm_values/traefik/crowdsec-bouncer.yaml"
      register: crowdsecbouncertraefikconfig
    - name: Configure crowdsec bouncer for traefik
      when: kubeconfig.stat.exists
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        state: present
        namespace: network-system
        src: "{{ crowdsecbouncertraefikconfig.stat.path }}"
        # validate:
        #   fail_on_error: false
    - name: Check for authentik config for traefik
      ansible.builtin.stat:
        path: "{{ playbook_dir }}/helm_values/traefik/authentik.yaml"
      register: authentiktraefikconfig
    - name: Configure authentik for traefik
      when: kubeconfig.stat.exists
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        state: present
        namespace: network-system
        src: "{{ authentiktraefikconfig.stat.path }}"
        # validate:
        #   fail_on_error: false

# Disabled, Looking into alternatives
# # Patch Operator
# - name: Patch Operator | Pre-steps
#   hosts: localhost
#   vars:
#     home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
#   tasks:
#     - name: Check for kubeconfig
#       ansible.builtin.stat:
#         path: "~/.kube/config"
#       register: kubeconfig
#     - name: Add patch operator repository
#       # when: kubeconfig.stat.exists
#       kubernetes.core.helm_repository:
#         kubeconfig: "{{ home }}/.kube/config"
#         name: patch-operator
#         repo_url: https://redhat-cop.github.io/patch-operator
#     - name: Create network-system namespace
#       # when: kubeconfig.stat.exists
#       kubernetes.core.k8s:
#         kubeconfig: "{{ home }}/.kube/config"
#         name: patch-operator-system
#         api_version: v1
#         kind: Namespace
#         state: present
#         definition:
#           annotations:
#             # cni.projectcalico.org/ipv4pools: '["system"]'
# - name: Patch Operator | Install
#   hosts: localhost
#   vars:
#     home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
#   tasks:
#     - name: Deploy Patch Operator chart
#       # when: kubeconfig.stat.exists
#       kubernetes.core.helm:
#         kubeconfig: "{{ home }}/.kube/config"
#         name: patch-operator
#         release_namespace: patch-operator-system
#         # renovate: registryUrl=https://redhat-cop.github.io/patch-operator
#         chart_ref: patch-operator/patch-operator
#         chart_version: v0.1.11
#         values:
#           enableMonitoring: false
#     - name: Install Patch ServiceAccount
#       kubernetes.core.k8s:
#         kubeconfig: "{{ home }}/.kube/config"
#         state: "present"
#         definition:
#           apiVersion: v1
#           kind: ServiceAccount
#           metadata:
#             name: patching-service-account
#             namespace: patch-operator-system
#     - name: Install Patch ServiceAccount ClusterRole
#       kubernetes.core.k8s:
#         kubeconfig: "{{ home }}/.kube/config"
#         state: "present"
#         definition:
#           apiVersion: rbac.authorization.k8s.io/v1
#           kind: ClusterRole
#           metadata:
#             name: service-account-modifier
#           rules:
#             - apiGroups: [""]
#               resources: ["serviceaccounts"]
#               verbs: ["get", "watch", "list", "update", "patch"]
#     - name: Install Patch ServiceAccount ClusterRole
#       kubernetes.core.k8s:
#         kubeconfig: "{{ home }}/.kube/config"
#         state: "present"
#         definition:
#           apiVersion: rbac.authorization.k8s.io/v1
#           kind: ClusterRoleBinding
#           metadata:
#             name: service-account-modifier-binding
#           subjects:
#             - kind: ServiceAccount
#               name: patching-service-account
#               namespace: patch-operator-system
#           roleRef:
#             kind: ClusterRole
#             name: service-account-modifier
#             apiGroup: rbac.authorization.k8s.io
#     - name: Wait for patch-operator manifest to complete
#       kubernetes.core.k8s_info:
#         kubeconfig: "{{ home }}/.kube/config"
#         kind: Deployment
#         namespace: patch-operator-system
#         label_selectors:
#           - app.kubernetes.io/instance = patch-operator
#         wait: true
#         wait_condition:
#           type: "Available"
#           status: "True"
#         wait_sleep: 5
#         wait_timeout: 300
#     - name: Install Image Pull Secret Patch
#       kubernetes.core.k8s:
#         kubeconfig: "{{ home }}/.kube/config"
#         state: "present"
#         definition:
#           apiVersion: redhatcop.redhat.io/v1alpha1
#           kind: Patch
#           metadata:
#             name: dockerhub-image-pull-secret-patch
#             namespace: patch-operator-system
#           spec:
#             serviceAccountRef:
#               name: patching-service-account
#             patches:
#               service-account-patch:
#                 targetObjectRef: 
#               apiVersion: v1
#               kind: ServiceAccount
#             patchType: application/strategic-merge-patch+json
#             patchTemplate: |
#               imagePullSecrets:
#                 - name: docker-registry-credentials

# Flux
- name: Bootstrap Flux
  hosts: localhost
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
    # kubeconfig: "{{ home }}/.kube/config"
  tasks:
    - name: Flux pre-checks
      ansible.builtin.command: |
        KUBECONFIG="{{ kubeconfig.stat.path }}" flux check --pre
      become: true
      register: flux_precheck_out
      changed_when: true
      failed_when:
        - flux_precheck_out.rc != 0
    - name: Check github secrets encrypted file
      ansible.builtin.stat:
        path: "{{ playbook_dir + '/secrets/github.sops.yaml.enc' }}"
      register: githubsecretsfile
    - name: Unencrypt Docker Secrets
      ansible.builtin.set_fact:
          github: "{{ lookup('file', playbook_dir + '/secrets/github.sops.yaml.enc') | community.sops.decrypt | ansible.builtin.from_yaml }}"
      when: githubsecretsfile.stat.exists
      failed_when: not githubsecretsfile.stat.exists
    - name: Flux bootstrap
      environment:
        GITHUB_TOKEN: "{{ (github | from_yaml).GITHUB_TOKEN }}"
      ansible.builtin.command: |
        KUBECONFIG="{{ kubeconfig.stat.path }}" flux bootstrap github \
          --owner=ryanheyser \
          --repository=homelab-ops \
          --branch main \
          --private=false \
          --personal \
          --network-policy=false
      register: flux_bootstrap_out
      changed_when: true
      failed_when:
        - flux_bootstrap_out.rc != 0

# Vault
- name: Install Vault
  hosts: localhost
  strategy: linear
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Add hashicorp repository
      when: kubeconfig.stat.exists
      kubernetes.core.helm_repository:
        kubeconfig: "{{ home }}/.kube/config"
        name: hashicorp
        repo_url: https://helm.releases.hashicorp.com
    - name: Create vault-system namespace
      when: kubeconfig.stat.exists
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        name: vault-system
        api_version: v1
        kind: Namespace
        state: present
        definition:
          annotations:
            cni.projectcalico.org/ipv4pools: '["system"]'
    - name: Deploy vault chart
      when: kubeconfig.stat.exists
      kubernetes.core.helm:
        kubeconfig: "{{ home }}/.kube/config"
        name: vault
        release_namespace: vault-system
        # renovate: registryUrl=https://helm.releases.hashicorp.com
        chart_ref: hashicorp/vault
        chart_version: 0.28.1
        values_files:
          - "{{ playbook_dir }}/ansible/helm_values/vault/values.yaml"
    - name: Deploy csi-secrets-store chart
      when: kubeconfig.stat.exists
      kubernetes.core.helm:
        kubeconfig: "{{ home }}/.kube/config"
        name: csi-secrets-store
        release_namespace: kube-system
        # renovate: registryUrl=https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts
        chart_ref: secrets-store-csi-driver/secrets-store-csi-driver
        chart_version: 1.4.5
        values_files:
          - "{{ playbook_dir }}/ansible/helm_values/vault/csivalues.yaml"
    - name: Check if Vault Operator Init File Exists
      ansible.builtin.stat:
        path: "{{ playbook_dir }}/secrets/vault-operator-init-secret.key"
      register: vault_operator_init_file
    - name: Initialize Vault
      # noqa: run-once[task]
      run_once: true
      kubernetes.core.k8s_exec:
        kubeconfig: "{{ home }}/.kube/config"
        namespace: vault-system
        pod: vault-0
        command: vault operator init -format=json -key-shares=10 -key-threshold=3 -recovery-shares=10 -recovery-threshold=3
      register: vault_operator_init
      when: >-
        vault_operator_init_file.stat.exists and
        kubeconfig.stat.exists
    - name: Write Vault Configuration to File
      # noqa: run-once[task]
      run_once: true
      ansible.builtin.copy:
        content: "{{ vault_operator_init.stdout }}"
        dest: "{{ playbook_dir }}/secrets/vault-operator-init-secret.key"
        mode: preserve
      when: >-
        vault_operator_init is defined and
        not vault_operator_init is skipped
      register: vault_operator_init_config
    - name: Write Vault Configuration to Encrypted File
      # noqa: run-once[task]
      run_once: true
      community.sops.sops_encrypt:
        content_json: "{{ vault_operator_init.stdout }}"
        path: "{{ playbook_dir }}/secrets/vault-operator-init-secret.key.enc"
        age: age_public_key.stdout
      when: >-
        vault_operator_init_config is defined and
        not vault_operator_init_config is skipped
    - name: Read Vault Configuration from File
      # noqa: run-once[task]
      run_once: true
      ansible.builtin.set_fact:
        vault_operator_init: "lookup('file', {{ playbook_dir }}'/secrets/vault-operator-init-secret.key')"
      when: vault_operator_init_file.stat.exists
    - name: Check Vault Status
      when: kubeconfig.stat.exists
      kubernetes.core.k8s_exec:
        kubeconfig: "{{ home }}/.kube/config"
        namespace: vault-system
        pod: vault-0
        command: vault status -format=json
      register: vault_status
- name: Unseal Vault
  hosts: localhost
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Unseal Vault 1
      when: >-
        vault_status.stdout is defined and
        (vault_status.stdout | from_json).sealed and
        kubeconfig.stat.exists
      kubernetes.core.k8s_exec:
        kubeconfig: "{{ home }}/.kube/config"
        namespace: vault-system
        pod: vault-0
        command: vault operator unseal {{ (vault_operator_init.stdout | from_json).unseal_keys_b64[0] }} -format=json
    - name: Unseal Vault 2
      when: >-
        vault_status.stdout is defined and
        (vault_status.stdout | from_json).sealed and
        kubeconfig.stat.exists
      kubernetes.core.k8s_exec:
        kubeconfig: "{{ home }}/.kube/config"
        namespace: vault-system
        pod: vault-0
        command: vault operator unseal {{ (vault_operator_init.stdout | from_json).unseal_keys_b64[1] }} -format=json
    - name: Unseal Vault 3
      when: >-
        vault_status.stdout is defined and
        (vault_status.stdout | from_json).sealed and
        kubeconfig.stat.exists
      kubernetes.core.k8s_exec:
        kubeconfig: "{{ home }}/.kube/config"
        namespace: vault-system
        pod: vault-0
        command: vault operator unseal {{ (vault_operator_init.stdout | from_json).unseal_keys_b64[2] }} -format=json
    - name: Check Vault Status Again
      kubernetes.core.k8s_exec:
        kubeconfig: "{{ home }}/.kube/config"
        namespace: vault-system
        pod: vault-0
        command: vault status -format=json
      register: vault_status_after_unseal
      failed_when: (vault_status_after_unseal | from_json).sealed
      when: >-
        kubeconfig.stat.exists
    - name: Check if Vault Operator Root File Exists
      ansible.builtin.stat:
        path: "{{ playbook_dir }}/secrets/vault-operator-root-generate.key"
      register: vault_operator_root_file
- name: Unseal Vault
  hosts: localhost
  strategy: linear
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Initialize Vault Root Token
      # noqa: run-once[task]
      run_once: true
      kubernetes.core.k8s_exec:
        kubeconfig: "{{ home }}/.kube/config"
        namespace: vault-system
        pod: vault-0
        command: vault operator generate-root -init -format=json
      register: vault_operator_root
      when: >-
        not vault_operator_root_file.stat.exists and
        kubeconfig.stat.exists
    - name: Write Vault Root Generation to File
      # noqa: run-once[task]
      run_once: true
      ansible.builtin.copy:
        content: vault_operator_root.stdout
        dest: "{{ playbook_dir }}/secrets/vault-operator-root-generate.key"
        mode: preserve
      when: >-
        vault_operator_root is defined and
        not vault_operator_root is skipped
    - name: Read Vault Root Token from File
      # noqa: run-once[task]
      run_once: true
      ansible.builtin.set_fact:
        vault_operator_root: "lookup('file', {{ playbook_dir }}'/secrets/vault-operator-root-generate.key')"
      when: vault_operator_root_file.stat.exists
    - name: Set Vault Root Token OTP
      # noqa: run-once[task]
      run_once: true
      ansible.builtin.set_fact:
        vault_operator_root_otp: "lookup('file', {{ playbook_dir }}'/secrets/vault-operator-root-generate.key')"
      when: vault_operator_root_file.stat.exists
    - name: Generate Root Token 1
      # noqa: run-once[task]
      run_once: true
      kubernetes.core.k8s_exec:
        kubeconfig: "{{ home }}/.kube/config"
        namespace: vault-system
        pod: vault-0
        command: >-
          vault operator generate-root -otp={{ (vault_operator_root_otp.stdout | from_json).otp }} \
            -decode={{ (vault_operator_init.stdout | from_json).unseal_keys_b64[0] }} -format=json
      when: >-
        vault_operator_root.stdout is defined and
        vault_operator_init.stdout is defined and
        not (vault_operator_root.stdout | from_json).complete and
        kubeconfig.stat.exists
      register: vault_operator_root
    - name: Generate Root Token 2
      # noqa: run-once[task]
      run_once: true
      kubernetes.core.k8s_exec:
        kubeconfig: "{{ home }}/.kube/config"
        namespace: vault-system
        pod: vault-0
        command: >-
          vault operator generate-root -otp={{ (vault_operator_root_otp.stdout | from_json).otp }} \
            -decode={{ (vault_operator_init.stdout | from_json).unseal_keys_b64[1] }} -format=json
      when: >-
        vault_operator_root.stdout is defined and
        vault_operator_init.stdout is defined and
        not (vault_operator_root.stdout | from_json).complete and
        kubeconfig.stat.exists
      register: vault_operator_root
    - name: Generate Root Token 3
      # noqa: run-once[task]
      run_once: true
      kubernetes.core.k8s_exec:
        kubeconfig: "{{ home }}/.kube/config"
        namespace: vault-system
        pod: vault-0
        command: >-
          vault operator generate-root -otp={{ (vault_operator_root_otp.stdout | from_json).otp }} \
            -decode={{ (vault_operator_init.stdout | from_json).unseal_keys_b64[2] }} -format=json
      when: >-
        vault_operator_root.stdout is defined and
        vault_operator_init.stdout is defined and
        not (vault_operator_root.stdout | from_json).complete and
        kubeconfig.stat.exists
      register: vault_operator_root
    - name: Write Vault Encrypted Root Token to File
      # noqa: run-once[task]
      run_once: true
      community.sops.sops_encrypt:
        content_json: "{{ vault_operator_root.stdout | to_json }}"
        path: "{{ playbook_dir }}/secrets/vault-operator-root-token.key.enc"
        age: age_public_key.stdout
      when: >-
        vault_operator_root.stdout is defined and
        vault_operator_init.stdout is defined and
        (vault_operator_root.stdout | from_json).complete
- name: Create Vault Token Secret
  hosts: localhost
  strategy: linear
  vars:
    home: "{{ lookup('ansible.builtin.env', 'HOME') }}"
  tasks:
    - name: Login to Vault
      # noqa: run-once[task]
      run_once: true
      kubernetes.core.k8s_exec:
        kubeconfig: "{{ home }}/.kube/config"
        namespace: vault-system
        pod: vault-0
        command: >-
          vault login -format=json -address=http://vault.vault-system.svc.cluster.local:8200 \
            {{ (vault_operator_init.stdout | from_json).root_token }}
      when: >-
        vault_operator_root.stdout is defined and
        vault_operator_init.stdout is defined and
        (vault_operator_root.stdout | from_json).complete and
        (vault_operator_root.stdout | from_json).encoded_token != "" and
        (vault_operator_init.stdout | from_json).root_token != "" and
        kubeconfig.stat.exists
      register: vault_login
      failed_when: (vault_login.stdout | from_json).auth.client_token == ""
    - name: Create Vault Token Secret
      # noqa: run-once[task]
      run_once: true
      kubernetes.core.k8s:
        kubeconfig: "{{ home }}/.kube/config"
        state: present
        definition:
          apiVersion: v1
          kind: Secret
          metadata:
            name: vault-token
            namespace: kube-system
          type: opaque
          data:
            token: "{{ (vault_operator_init.stdout | from_json).root_token | b64encode }}"
      when: >-
        kubeconfig.stat.exists and
        vault_operator_init.stdout is defined
      failed_when: >-
        {{(vault_operator_init.stdout | from_json).root_token | b64encode}} == ""
